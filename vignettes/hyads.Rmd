---
title: "Running HyADS for many sources"
author: "Lucas Henneman"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Running HyADS for many sources}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(
echo = T, 
results = 'hide',
collapse = TRUE,
comment = "#>"
)
```


Here we provide an example workflow of how to run the HyADS model using `hyspdisp`. This vignette provides instruction on how to run the model and ensure the model runs completed. Subsequent vignettes will describe combining, manipulating, and interpreting results.

```{r install, message = FALSE, warning = FALSE}
devtools::install_github("lhenneman/hyspdisp@dev2")
library( hyspdisp)
```


## Getting started - downloading important files
Several files are required to run the functions in `hyspdisp`. This section presents procedures to download these.

#### ZIP code-ZCTA crosswalk file
The ZIP code linkage procedure requires a ZCTA-to-ZIP code Crosswalk file. ZCTAs are not exact geographic matches to ZIP codes, and multiple groups compile and maintain Crosswalk files. One example is the Crosswalk mainted by UDS Mapper. It can be retrieved and its names changed for consistency with ```hyspdisp``` functions with the following commands:
```{r crosswalk}
library( openxlsx)
crosswalkin <- data.table( read.xlsx( xlsxFile =
                                        "https://www.udsmapper.org/docs/zip_to_zcta_2017.xlsx"))
setnames( crosswalkin, 
          old = "ZIP_CODE", 
          new = "ZIP")
```

#### ZCTA shapefile
Equally important is the ZCTA shapefile. These are available from multiple locations, including the [US census website](http://www2.census.gov/geo/tiger/GENZ2017/shp/cb_2017_us_zcta510_500k.zip). Code below downloads the file from the FTP site, which is more stable but slower. A third alternative (not shown) is to use the `tigris` r library. Download the file, unzip it, and read in the shapefile (here I've unzipped it to a directory in my Desktop). Loading the file may take a few moments.
```{r zcta_shapefile}
# Define the storage directory, create it if it does not exist
zcta_dir <- file.path('~', 'Desktop', 'run_hyspdisp', 'zcta_500k')
zcta_file <- file.path( zcta_dir, 'cb_2017_us_zcta510_500k.zip')
dir.create( zcta_dir, 
            recursive = T, 
            showWarnings = F)

# Download the file if it is not present 
zcta_file_url <- 'ftp://ftp2.census.gov/geo/tiger/GENZ2017/shp/cb_2017_us_zcta510_500k.zip'
if( !file.exists( zcta_file)){
  download.file( url = zcta_file_url,
                 destfile = zcta_file)
  unzip( zcta_file, exdir = zcta_dir)
}

# Load the ZCTA shapefile using the shapefile command
zcta_shapefile <- file.path( zcta_dir, 'cb_2017_us_zcta510_500k.shp')
zcta <- shapefile( x = zcta_shapefile)
```

It is recommended to transform the ZCTA shapefile to a known projection to maintain consistency throughout the allocation process. Lat-lon projections are preferred, such as the [North American Albers Equal Area Conic](https://epsg.io/102008):
```{r proj_shapefile}
p4s <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m"
zcta.trans <- spTransform( x = zcta, 
                           CRSobj = p4s)
```

#### Monthly global planetary boundary layer
The final input file is the monthly mean boundary layer heights. For years up to and including 2012, these are available in a single file from [NOAA's Earth System Research Library](https://www.esrl.noaa.gov/psd/data/gridded/data.20thC_ReanV2.monolevel.mm.html). For years 2013 and later (and available for years 2008 and on), you may use the [NCAR/UCAR data archive](https://rda.ucar.edu/thredds/catalog/files/e/ds630.1/e5.mnth.mean.an.sfc/catalog.html). The NCAR/UCAR annual files must be downloaded one by one. Below I've included examples for how to download the two types of files if they are not already in the file path.
```{r pbl_height_download}
# Define a directory, create it if it does not exist
hpbl_dir <- file.path('~', 'Desktop', 'run_hyspdisp', 'hpbl')
dir.create( hpbl_dir, 
            recursive = T, 
            showWarnings = F)

# Download the PBL file from NOAA for years up to and including
hpbl_file_NOAA <- file.path( hpbl_dir, 'hpbl.mon.mean.nc')
url_NOAA <- "ftp://ftp.cdc.noaa.gov/Datasets/20thC_ReanV2/Monthlies/gaussian/monolevel/hpbl.mon.mean.nc"
if( !file.exists( hpbl_file_NOAA))
  download.file( url = url_NOAA,
                 destfile = hpbl_file_NOAA)

# Download the PBL file from NCAR for 2013
hpbl_file_NCAR2013 <- file.path( hpbl_dir, 'hpbl.mon.mean.nc_2013.grb')
url_NCAR <- "https://rda.ucar.edu/thredds/catalog/files/e/ds630.1/e5.mnth.mean.an.sfc/2013/catalog.html?dataset=files/e/ds630.1/e5.mnth.mean.an.sfc/2013/e5.mnth.mean.an.sfc.128_159_blh.regn320sc.2013010100_2013120100.grb"
if( !file.exists( hpbl_file_NCAR2013))
  download.file( url = url_NCAR,
                 destfile = hpbl_file_NCAR2013)

```

Both files are read in using the brick command, and rotated to match the lat-lon projection applied above. Before reading in, it is necessary to set the system time zone to UTC so that the dates are formatted correctly in the raster files.  For the NCAR boundary layer files, it is not neccesary to set the `varname` argument in the `brick` function.
```{r pbl_height_read, message = FALSE}
Sys.setenv(TZ='UTC')
hpbl_rasterin <- brick( x = hpbl_file_NOAA, 
                                varname = 'hpbl' )
```

## Setting up the inputs
Functions in `hyspdisp` require specific inputs. Their setups are detailed in this section.

#### Selecting units
Select power plants to run. In this case, we'll use the two units in 2005 with the greatest SOx emissions. This package contains annual emissions and stack height data from [EPA's Air Markets Program Data](https://ampd.epa.gov/ampd/) and the [Energy Information Agency] (https://www.eia.gov/electricity/data/eia860/) for 2005, 2006, and 2012. [PLUG CHRISTINE's DATA PAPER].

Many of the units in the provided datasets do not have stack height data. In these cases, it is suggested in [Henneman et al. (2019)](https://www.sciencedirect.com/science/article/pii/S1352231019300731) to fill with the average stack height of all units.

```{r select_unit}
data( units2005)
units2005 <- data.table( units2005)[, V1 := NULL]
units.run <- units2005[ order( -SOx)][1:2]
units.run
```
```{r printunits, echo = FALSE, results = 'markup'}
knitr::kable( units.run)
```

#### Designate directories for storing output files
`hyspdisp` creates multiple file types that store information important in interpreting the results. It is recommended to define the locations beforehand so that you:

- can access the files later
- can ensure there is enough storage space available

In general, `hyspdisp` functions will create the directories if they do not already exist. If they are not defined, they will be created in the working directory. The following directories, as referenced in the functions below in this document, will contain:

* `proc_dir`: the overarching directory containing each of the subdirectories, and the working directory for HYSPLIT
 * `hysraw_dir`: raw hyspdisp output (one file for each emissions event)
 * `ziplink_dir`: files containing ZIP code linkages
 * `meteo_dir`: (reanalysis) meteorology files
 * `rdata_dir`: RData files containing HyADS source-receptor matrices

```{r out_directories,results = 'markup'}
## proc_dir defines where runs happen and output is saved
## up_dir is where input data is saved (upper file structure)
proc_dir <- '~/Desktop/run_hyspdisp'
hysraw_dir <- '~/Desktop/run_hyspdisp/output_hysplit'
ziplink_dir <- '~/Desktop/run_hyspdisp/output_ziplinks'
meteo_dir <- '~/Desktop/run_hyspdisp/meteo'
rdata_dir <- '~/Desktop/hyads_test/output_rdata'
```


#### Download reanalysis meteorology files
While SplitR (the R package that calls HYSPLIT) includes code that checks if the appropriate metorological files are downloaded, it is recommended to download needed files explicitely before the *first* run of`hyspdisp_fac_model_parallel` because the parallel code does not handle the download well split over multiple cores. Below is shown code to test for the three meteorology files needed for the present run, and download them if they are not already in the `meteo_dir`. The reanalysis met files are about 120 MB each. 
```{r met_files,results = 'markup'}
metfiles.vignette <- c( 'RP200412.gbl',
                        'RP200501.gbl',
                        'RP200502.gbl')
metfiles.vignette <- metfiles.vignette[!(metfiles.vignette %in% list.files(meteo_dir))]
if( length( metfiles.vignette) > 0)
    get_met_reanalysis(files = metfiles.vignette, 
                       path_met_files = meteo_dir)

```

#### Define time periods to be run
To define an object that includes all emission events in a given time period, we can use the helper function `define_inputtimes`. This takes as inputs a starting and ending day, and outputs a table of value whose rows will later correspond to inputs into the main `hyspdisp` worker functions. The following command combines the units defined above with four times a day in January 2005. The resulting data.table's column names are printed in the table below.
```{r define_inputs}
## combine dates and hours
input_refs <- define_inputs( units = units.run,
                             startday = '2005-01-01',
                             endday = '2005-01-31')
names( input_refs)
```
```{r results = 'markup', echo = F}
knitr::kable( names( input_refs))
```

## Run hyspdisp worker functions
The following examples show how to run a small subset (chosen to provide workable examples on a laptop) of emissions events, link to ZIP codes, and plot the results.

#### Run HYSPLIT
The `hyspdisp_fac_model_parallel` function runs HYSPLIT for each emissions event described by the rows of the `run_ref_tab`. Inputs defined above, including the projected ZCTA shapefile (`zcta.trans`), the ZIP-ZCTA crosswalk (`crosswalkin`), the planetary boundary layer raster (`hpbl_rasterin`), and working directories are necessary in the function call. With `link2zip = T` you can link dispersion patterns from individual emissions events to ZIP codes; this, however, somewhat violates the spirit of HyADS, since the large number of emissions events is used as a check on some other uncertainties introduced by the simplifying assumptions. 

Here it's run for the first five (`X = 1:5`) rows of the `input_refs` table. The results will be saved in the `hysraw_dir`. `mc.cores` is an option in the mcapply function that controls the number of cores should be used. The output describes where the files are saved, or the reasons they were not run.

```{r hyspdisp_fac}
library(parallel)
hysp_raw <- mclapply( X = 1:5,
                      FUN = hyspdisp_fac_model_parallel,
                      run_ref_tab = input_refs,
                      zcta2 = zcta.trans,
                      crosswalk = crosswalkin,
                      hpbl_raster = hpbl_rasterin,
                      prc_dir = proc_dir,
                      hyo_dir = hysraw_dir,
                      met_dir = meteo_dir,
                      mc.cores = 4)
hysp_raw[1]
```
```{r hyspdisp_fac_print, echo = FALSE, results = 'markup'}
library(parallel)
knitr::kable( head( hysp_raw, 1))
```

#### Link results to ZIP codes
Most current implementations of HyADS, instead of linking dispersion patterns from individual emissions events, link the patterns by month. With the `hysdisp_zip_link` function, users can link all air parcels to ZIP codes by month for an individual unit. Here, we define the variables `yearmons` with combinations of years and months. `hysdisp_zip_link` reads in all the relevant files (i.e., those that correspond to the provided `month_YYYYMM` and `unit`) produced by the `hyspdisp_fac_model_parallel` function and saved to the `hyo_dir`, then links them to ZIP codes. The result is data.table of ZIP codes and relative contributions `N`, and an identical `.csv` file is saved to the `zpc_dir`. `N` is not weighted by the `unit`'s emissions. `hyspdisp_zip_link` is parallelizable using `mclapply` or similar, and is run for a single unit (we define the `link_unit` as the first unit in `unit.run`.)
```{r ziplink, results = 'markup', message = FALSE}
link_unit <- input_refs[1]

yearmons <- paste0( 2005, 1:12)
linked_zips <- hyspdisp_zip_link( month_YYYYMM = yearmons[1],
                                  unit = link_unit,
                                  hpbl_raster = hpbl_rasterin,
                                  zcta2 = zcta.trans,
                                  crosswalk = crosswalkin,
                                  prc_dir = proc_dir,
                                  zpc_dir = ziplink_dir,
                                  hyo_dir = hysraw_dir)
```

#### Plot the results
At this point, it makes sense to take a look at the results to see which ZIP codes are impacted by emissions from the facility whose impacts we just modeled. To acheive this, we can use the `plot_hyspdisp` function, which takes as input a data.table (or data.frame) linked to simple features ZIP code data. We can use the `st_read` function from the `sf` package to read in the same ZCTA shapefile we used to link the HYSPLIT output to ZIP codes. Next, we merge the `sf` ZIP code object to both the crosswalk file and the `linked_zips` object created above, then feed the merged dataset to the `plot_hyspdisp` function, which returns a ggplot object.

```{r plot_ziplinks, results = 'markup', message = FALSE, warning = FALSE, fig.width = 7}
library(sf)
library(viridis)
library(ggplot2)
library(scales)

zips.sf <- st_read(zcta_shapefile)
setnames( zips.sf, 'ZCTA5CE10', 'ZCTA')

zips_crosswalk.sf <- merge( zips.sf, 
                            crosswalkin, 
                            by = "ZCTA", all = F, allow.cartesian = TRUE) 

zip_dataset_sf <- data.table( merge( zips_crosswalk.sf, 
                                     linked_zips, 
                                     by = c('ZIP'), all.y = T))

ziplink_plot <- plot_hyspdisp(hyspdisp_out.sf = zip_dataset_sf,
                              metric = 'N',
                              plot.title = paste('Partial Jan. 2005 HyADS raw exposure, uID:', 
                                                 link_unit[, ID]),
                              legend.title = 'HyADS raw exposure',
                              facility.loc = data.table( x = link_unit[, Longitude],
                                                         y = link_unit[, Latitude]))
ziplink_plot
```

#### Combine all results into RData file
So far, we have run two worker functions important to the implementation of HyADS:

 - `hyspdisp_fac_model_parallel`: ran HYSPLIT for unit-date-time emission event combinations and saved an output file for each 
  - `hyspdisp_zip_link`: gathered output files from `hyspdisp_fac_model_parallel`, grouped them by month and unit, and linked the HYSPLIT parcel locations contained in the files to ZIP codes.

In practice, these two worker functions may need to be implemented in parallel R sessions on a cluster to improve efficiency. It helps for future analyses to gather the relevant monthly files and save them as a single RData file. This is made possible by `combine_monthly_ziplinks`, which saves an RData file of annual monthly results to the `rda_dir`.
```{r combine_ziplinks}
combined_ziplinks <- combine_monthly_ziplinks( month_YYYYMMs = yearmons,
                                               zpc_dir = ziplink_dir,
                                               rda_dir = rdata_dir)
names( combined_ziplinks)
```
```{r combine_ziplinks_print, results = 'markup', echo = F}
knitr::kable( names( combined_ziplinks))
```






